{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, cohen_kappa_score,\n",
    "    precision_recall_curve, roc_curve, classification_report\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# CUDA/GPU Optimization Configuration\n",
    "def setup_gpu_optimization():\n",
    "    \"\"\"Enhanced GPU setup with CUDA optimizations\"\"\"\n",
    "    print(\"Setting up GPU/CUDA optimizations...\")\n",
    "    \n",
    "    # Enable GPU growth and mixed precision\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for all GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                \n",
    "            # Set memory limit if needed (optional - uncomment if you want to limit GPU memory)\n",
    "            # tf.config.experimental.set_memory_limit(gpus[0], 8192)  # 8GB limit\n",
    "            \n",
    "            print(f\"✓ Found {len(gpus)} GPU(s)\")\n",
    "            print(f\"✓ GPU devices: {[gpu.name for gpu in gpus]}\")\n",
    "            \n",
    "            # Enable mixed precision for faster training on modern GPUs\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print(\"✓ Mixed precision (float16) enabled for faster training\")\n",
    "            \n",
    "            # Enable XLA compilation for better performance\n",
    "            tf.config.optimizer.set_jit(True)\n",
    "            print(\"✓ XLA JIT compilation enabled\")\n",
    "            \n",
    "            # Set GPU as default device\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            \n",
    "            return True, gpus\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU setup error: {e}\")\n",
    "            return False, []\n",
    "    else:\n",
    "        print(\"⚠ No GPU found, using CPU\")\n",
    "        return False, []\n",
    "\n",
    "# Initialize GPU\n",
    "HAS_GPU, GPU_DEVICES = setup_gpu_optimization()\n",
    "\n",
    "# Enhanced Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64 if HAS_GPU else 16  # Larger batch size for GPU\n",
    "EPOCHS = 50\n",
    "PREFETCH_BUFFER = tf.data.AUTOTUNE\n",
    "NUM_PARALLEL_CALLS = tf.data.AUTOTUNE\n",
    "\n",
    "# Enable tensor core usage on modern GPUs (V100, A100, RTX series)\n",
    "if HAS_GPU:\n",
    "    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "class RetinaMaskLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"CUDA-optimized custom layer for retina masking\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(RetinaMaskLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(RetinaMaskLayer, self).build(input_shape)\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs):\n",
    "        feature_maps, mask = inputs\n",
    "        \n",
    "        # Use GPU-optimized operations\n",
    "        with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "            # Get feature map dimensions\n",
    "            feature_shape = tf.shape(feature_maps)\n",
    "            batch_size = feature_shape[0]\n",
    "            height = feature_shape[1] \n",
    "            width = feature_shape[2]\n",
    "            channels = feature_shape[3]\n",
    "            \n",
    "            # Efficiently handle mask dimensions\n",
    "            if len(mask.shape) == 5:\n",
    "                mask = tf.squeeze(mask, axis=[-1, -2])\n",
    "            elif len(mask.shape) == 4 and mask.shape[-1] == 1:\n",
    "                mask = tf.squeeze(mask, axis=-1)\n",
    "            \n",
    "            # GPU-optimized resize with bilinear interpolation\n",
    "            mask_resized = tf.image.resize(\n",
    "                tf.expand_dims(mask, axis=-1), \n",
    "                [height, width], \n",
    "                method='bilinear',\n",
    "                antialias=True\n",
    "            )\n",
    "            \n",
    "            # Efficient broadcasting using tf.broadcast_to\n",
    "            mask_broadcasted = tf.broadcast_to(\n",
    "                mask_resized, \n",
    "                [batch_size, height, width, channels]\n",
    "            )\n",
    "            \n",
    "            # Element-wise multiplication (GPU-accelerated)\n",
    "            masked_features = tf.multiply(feature_maps, mask_broadcasted)\n",
    "            \n",
    "        return masked_features\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(RetinaMaskLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "@tf.function\n",
    "def create_retina_mask_gpu(image_tensor):\n",
    "    \"\"\"GPU-accelerated retina mask creation using TensorFlow operations\"\"\"\n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        # Convert to grayscale using weighted sum (faster than cv2)\n",
    "        if len(image_tensor.shape) == 3 and image_tensor.shape[-1] == 3:\n",
    "            # Standard RGB to grayscale weights\n",
    "            weights = tf.constant([0.299, 0.587, 0.114], dtype=tf.float32)\n",
    "            gray = tf.reduce_sum(image_tensor * weights, axis=-1)\n",
    "        else:\n",
    "            gray = tf.squeeze(image_tensor)\n",
    "        \n",
    "        # Normalize to 0-255 range\n",
    "        gray = tf.cast(gray, tf.float32)\n",
    "        if tf.reduce_max(gray) <= 1.0:\n",
    "            gray = gray * 255.0\n",
    "        \n",
    "        # GPU-optimized Gaussian blur using separable filters\n",
    "        kernel_size = 5\n",
    "        sigma = 1.0\n",
    "        \n",
    "        # Create 1D Gaussian kernel\n",
    "        x = tf.range(-kernel_size//2 + 1, kernel_size//2 + 1, dtype=tf.float32)\n",
    "        kernel_1d = tf.exp(-0.5 * tf.square(x / sigma))\n",
    "        kernel_1d = kernel_1d / tf.reduce_sum(kernel_1d)\n",
    "        \n",
    "        # Reshape for convolution\n",
    "        kernel_1d = tf.reshape(kernel_1d, [kernel_size, 1, 1, 1])\n",
    "        \n",
    "        # Apply separable Gaussian blur\n",
    "        gray_expanded = tf.expand_dims(tf.expand_dims(gray, 0), -1)\n",
    "        blurred = tf.nn.conv2d(gray_expanded, kernel_1d, strides=[1,1,1,1], padding='SAME')\n",
    "        kernel_1d_t = tf.transpose(kernel_1d, [1, 0, 2, 3])\n",
    "        blurred = tf.nn.conv2d(blurred, kernel_1d_t, strides=[1,1,1,1], padding='SAME')\n",
    "        blurred = tf.squeeze(blurred, [0, 3])\n",
    "        \n",
    "        # Thresholding (GPU-accelerated)\n",
    "        threshold = 20.0\n",
    "        binary_mask = tf.cast(blurred > threshold, tf.float32)\n",
    "        \n",
    "        # Morphological operations using GPU-optimized erosion/dilation\n",
    "        kernel = tf.ones([7, 7], dtype=tf.float32)\n",
    "        kernel_expanded = tf.expand_dims(tf.expand_dims(kernel, -1), -1)\n",
    "        \n",
    "        # Morphological close operation\n",
    "        mask_expanded = tf.expand_dims(tf.expand_dims(binary_mask, 0), -1)\n",
    "        \n",
    "        # Dilation followed by erosion (morphological close)\n",
    "        dilated = tf.nn.dilation2d(\n",
    "            mask_expanded, kernel_expanded, \n",
    "            strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1,1,1,1]\n",
    "        )\n",
    "        closed = tf.nn.erosion2d(\n",
    "            dilated, kernel_expanded,\n",
    "            strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1,1,1,1]\n",
    "        )\n",
    "        \n",
    "        # Erosion followed by dilation (morphological open)\n",
    "        eroded = tf.nn.erosion2d(\n",
    "            closed, kernel_expanded,\n",
    "            strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1,1,1,1]\n",
    "        )\n",
    "        opened = tf.nn.dilation2d(\n",
    "            eroded, kernel_expanded,\n",
    "            strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1,1,1,1]\n",
    "        )\n",
    "        \n",
    "        mask = tf.squeeze(opened, [0, 3])\n",
    "        \n",
    "        # Create circular fallback mask if needed\n",
    "        h, w = tf.shape(mask)[0], tf.shape(mask)[1]\n",
    "        center_y, center_x = h // 2, w // 2\n",
    "        radius = tf.minimum(h, w) // 3\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        y_coords = tf.range(h, dtype=tf.float32)\n",
    "        x_coords = tf.range(w, dtype=tf.float32)\n",
    "        y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "        \n",
    "        # Calculate distance from center\n",
    "        distances = tf.sqrt(\n",
    "            tf.square(y_grid - tf.cast(center_y, tf.float32)) + \n",
    "            tf.square(x_grid - tf.cast(center_x, tf.float32))\n",
    "        )\n",
    "        \n",
    "        circular_mask = tf.cast(distances <= tf.cast(radius, tf.float32), tf.float32)\n",
    "        \n",
    "        # Use processed mask if it has substantial content, otherwise use circular mask\n",
    "        mask_area = tf.reduce_sum(mask)\n",
    "        total_area = tf.cast(h * w, tf.float32)\n",
    "        mask_ratio = mask_area / total_area\n",
    "        \n",
    "        final_mask = tf.cond(\n",
    "            mask_ratio > 0.1,  # If mask covers more than 10% of image\n",
    "            lambda: mask,\n",
    "            lambda: circular_mask\n",
    "        )\n",
    "        \n",
    "        return final_mask\n",
    "\n",
    "# GPU-optimized data pipeline\n",
    "def create_optimized_dataset_with_masks(dataframe, batch_size, shuffle=False, cache=True):\n",
    "    \"\"\"Create highly optimized TensorFlow dataset with GPU acceleration\"\"\"\n",
    "    \n",
    "    @tf.function\n",
    "    def load_and_preprocess_with_mask(path, label):\n",
    "        # GPU-accelerated image loading and preprocessing\n",
    "        with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "            # Load image\n",
    "            image_raw = tf.io.read_file(path)\n",
    "            image_decoded = tf.image.decode_jpeg(image_raw, channels=3)  # Changed to decode_jpeg for better compatibility\n",
    "            image_resized = tf.image.resize(\n",
    "                image_decoded, \n",
    "                [IMG_SIZE, IMG_SIZE],\n",
    "                method='bilinear',\n",
    "                antialias=True\n",
    "            )\n",
    "            image_float = tf.cast(image_resized, tf.float32)\n",
    "            \n",
    "            # DenseNet preprocessing\n",
    "            image_preprocessed = tf.keras.applications.densenet.preprocess_input(image_float)\n",
    "            \n",
    "            # Generate mask using GPU-optimized function\n",
    "            mask = create_retina_mask_gpu(image_resized)\n",
    "            mask = tf.expand_dims(mask, axis=-1)\n",
    "            \n",
    "            return {'image': image_preprocessed, 'mask': mask}, label\n",
    "    \n",
    "    # Create optimized dataset pipeline\n",
    "    paths = tf.constant(dataframe['filepath'].values)\n",
    "    labels = tf.constant(dataframe['label'].values, dtype=tf.int32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    # Optimize dataset pipeline\n",
    "    dataset = dataset.map(\n",
    "        load_and_preprocess_with_mask, \n",
    "        num_parallel_calls=NUM_PARALLEL_CALLS,\n",
    "        deterministic=False  # Allow non-deterministic order for better performance\n",
    "    )\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()  # Cache processed data in memory\n",
    "    \n",
    "    if shuffle:\n",
    "        # Use larger shuffle buffer for better randomization\n",
    "        buffer_size = min(len(dataframe), 10000)\n",
    "        dataset = dataset.shuffle(buffer_size, seed=42, reshuffle_each_iteration=True)\n",
    "    \n",
    "    # Batch and prefetch optimizations\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "    dataset = dataset.prefetch(PREFETCH_BUFFER)\n",
    "    \n",
    "    # Enable optimization\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_optimization.map_paralization = True\n",
    "    options.experimental_optimization.parallel_batch = True\n",
    "    options.threading.private_threadpool_size = 8\n",
    "    dataset = dataset.with_options(options)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def create_retina_focused_densenet_gpu():\n",
    "    \"\"\"Create GPU-optimized DenseNet model with retina masking\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()  # Clear memory\n",
    "    \n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        # Input layers\n",
    "        image_input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='image')\n",
    "        mask_input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1), name='mask')\n",
    "        \n",
    "        # Base DenseNet121 with optimizations\n",
    "        base_model = DenseNet121(\n",
    "            weights=\"imagenet\", \n",
    "            include_top=False,\n",
    "            input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "            pooling=None\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Get feature maps\n",
    "        feature_maps = base_model(image_input, training=False)\n",
    "        \n",
    "        # Apply retina mask\n",
    "        masked_features = RetinaMaskLayer()([feature_maps, mask_input])\n",
    "        \n",
    "        # Enhanced attention mechanism with spatial attention\n",
    "        # Channel attention\n",
    "        channel_attention = tf.keras.layers.GlobalAveragePooling2D()(masked_features)\n",
    "        channel_attention = tf.keras.layers.Dense(\n",
    "            masked_features.shape[-1] // 8, \n",
    "            activation='relu',\n",
    "            dtype='float32'\n",
    "        )(channel_attention)\n",
    "        channel_attention = tf.keras.layers.Dense(\n",
    "            masked_features.shape[-1], \n",
    "            activation='sigmoid',\n",
    "            dtype='float32'\n",
    "        )(channel_attention)\n",
    "        channel_attention = tf.keras.layers.Reshape((1, 1, masked_features.shape[-1]))(channel_attention)\n",
    "        \n",
    "        # Apply channel attention\n",
    "        channel_refined = tf.keras.layers.Multiply()([masked_features, channel_attention])\n",
    "        \n",
    "        # Spatial attention\n",
    "        spatial_attention = tf.keras.layers.Conv2D(\n",
    "            1, (7, 7), \n",
    "            padding='same',\n",
    "            activation='sigmoid', \n",
    "            dtype='float32',\n",
    "            name='spatial_attention'\n",
    "        )(channel_refined)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        attended_features = tf.keras.layers.Multiply()([channel_refined, spatial_attention])\n",
    "        \n",
    "        # Advanced pooling strategy\n",
    "        # Global Average Pooling\n",
    "        gap = tf.keras.layers.GlobalAveragePooling2D()(attended_features)\n",
    "        \n",
    "        # Global Max Pooling\n",
    "        gmp = tf.keras.layers.GlobalMaxPooling2D()(attended_features)\n",
    "        \n",
    "        # Concatenate different pooling strategies\n",
    "        x = tf.keras.layers.Concatenate()([gap, gmp])\n",
    "        \n",
    "        # Enhanced classification head\n",
    "        x = tf.keras.layers.Dropout(0.4)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\", dtype='float32')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(256, activation=\"relu\", dtype='float32')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Output layer with proper dtype for mixed precision\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            5, \n",
    "            activation=\"softmax\", \n",
    "            dtype='float32',  # Ensure float32 for final output\n",
    "            name='predictions'\n",
    "        )(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.Model(inputs=[image_input, mask_input], outputs=outputs)\n",
    "    \n",
    "    # Optimized compiler settings\n",
    "    if HAS_GPU:\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7,\n",
    "            clipnorm=1.0  # Gradient clipping for mixed precision\n",
    "        )\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "        run_eagerly=False  # Ensure graph mode for better GPU performance\n",
    "    )\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Enhanced evaluation with GPU optimization\n",
    "@tf.function\n",
    "def predict_batch_gpu(model, batch_data):\n",
    "    \"\"\"GPU-optimized batch prediction\"\"\"\n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        return model(batch_data, training=False)\n",
    "\n",
    "def evaluate_model_5_class_gpu(model, test_set, model_name=\"Model\"):\n",
    "    \"\"\"GPU-optimized evaluation function\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name} on GPU...\")\n",
    "    \n",
    "    y_true, y_pred_probs = [], []\n",
    "    \n",
    "    # Use GPU for batch predictions\n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        for batch_num, (batch_data, labels) in enumerate(test_set):\n",
    "            print(f\"\\rEvaluating batch {batch_num + 1}\", end='')\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            \n",
    "            # GPU-optimized prediction\n",
    "            if HAS_GPU:\n",
    "                predictions = predict_batch_gpu(model, batch_data)\n",
    "                y_pred_probs.extend(predictions.numpy())\n",
    "            else:\n",
    "                predictions = model.predict(batch_data, verbose=0)\n",
    "                y_pred_probs.extend(predictions)\n",
    "    \n",
    "    print()  # New line\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    class_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    \n",
    "    print(f\"\\n{model_name} Results (5-Class Classification):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(\"Predicted ->\")\n",
    "    print(\"True |\")\n",
    "    print(\"     v\")\n",
    "    \n",
    "    header = \"    \" + \"\".join([f\"{i:>12}\" for i in range(5)])\n",
    "    print(header)\n",
    "    for i, row in enumerate(conf_matrix):\n",
    "        row_str = f\"{i:>2}: \" + \"\".join([f\"{val:>12}\" for val in row])\n",
    "        print(row_str)\n",
    "    \n",
    "    # Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print(f\"\\nCohen's Kappa Score: {kappa:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': accuracy,\n",
    "        'kappa_score': kappa,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_probs': y_pred_probs\n",
    "    }\n",
    "\n",
    "# GPU Memory monitoring\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor GPU memory usage\"\"\"\n",
    "    if HAS_GPU:\n",
    "        try:\n",
    "            gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            current_mb = gpu_info['current'] / (1024**2)\n",
    "            peak_mb = gpu_info['peak'] / (1024**2)\n",
    "            print(f\"GPU Memory - Current: {current_mb:.1f}MB, Peak: {peak_mb:.1f}MB\")\n",
    "        except:\n",
    "            print(\"Could not retrieve GPU memory info\")\n",
    "\n",
    "def load_and_prepare_data(base_dir, train_csv_path, images_folder_name=\"images\"):\n",
    "    \"\"\"\n",
    "    Load and prepare data with new structure where all images are in one folder\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory path\n",
    "        train_csv_path: Path to the CSV file containing image_id and level information\n",
    "        images_folder_name: Name of the folder containing all images (default: \"images\")\n",
    "    \n",
    "    Returns:\n",
    "        prepared DataFrame with filepaths and labels\n",
    "    \"\"\"\n",
    "    print(\"Loading and preparing dataset with new structure...\")\n",
    "    \n",
    "    # Load CSV file\n",
    "    if not os.path.exists(train_csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {train_csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    print(f\"Loaded CSV with {len(df)} entries\")\n",
    "    print(f\"CSV columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['image_id', 'level']  # Updated column names\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            # Try alternative column names\n",
    "            if col == 'image_id' and 'id_code' in df.columns:\n",
    "                df['image_id'] = df['id_code']\n",
    "                print(f\"Using 'id_code' as 'image_id'\")\n",
    "            elif col == 'level' and 'diagnosis' in df.columns:\n",
    "                df['level'] = df['diagnosis']\n",
    "                print(f\"Using 'diagnosis' as 'level'\")\n",
    "            else:\n",
    "                raise ValueError(f\"Required column '{col}' not found in CSV. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Create label column\n",
    "    df[\"label\"] = df[\"level\"]\n",
    "    \n",
    "    # Define class mappings\n",
    "    class_names_5 = {\n",
    "        0: \"No DR\",\n",
    "        1: \"Mild DR\", \n",
    "        2: \"Moderate DR\",\n",
    "        3: \"Severe DR\",\n",
    "        4: \"Proliferative DR\"\n",
    "    }\n",
    "    \n",
    "    df[\"class_name\"] = df[\"label\"].map(class_names_5)\n",
    "    \n",
    "    # Build image file paths\n",
    "    images_dir = os.path.join(base_dir, images_folder_name)\n",
    "    \n",
    "    if not os.path.exists(images_dir):\n",
    "        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "    \n",
    "    print(f\"Looking for images in: {images_dir}\")\n",
    "    \n",
    "    # Try different image extensions\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.JPG', '.JPEG', '.PNG']\n",
    "    \n",
    "    def find_image_path(image_id):\n",
    "        \"\"\"Find the actual image file with correct extension\"\"\"\n",
    "        for ext in image_extensions:\n",
    "            filepath = os.path.join(images_dir, f\"{image_id}{ext}\")\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "        return None\n",
    "    \n",
    "    # Create file paths\n",
    "    print(\"Mapping image IDs to file paths...\")\n",
    "    df[\"filepath\"] = df[\"image_id\"].apply(find_image_path)\n",
    "    \n",
    "    # Check for missing files\n",
    "    missing_files = df[\"filepath\"].isnull().sum()\n",
    "    if missing_files > 0:\n",
    "        print(f\"Warning: {missing_files} images not found\")\n",
    "        print(\"Sample missing image IDs:\")\n",
    "        missing_ids = df[df[\"filepath\"].isnull()][\"image_id\"].head(10).tolist()\n",
    "        print(missing_ids)\n",
    "        \n",
    "        # List some actual files in the directory for debugging\n",
    "        actual_files = os.listdir(images_dir)[:10]\n",
    "        print(f\"Sample files in directory: {actual_files}\")\n",
    "        \n",
    "        # Remove rows with missing files\n",
    "        df = df.dropna(subset=['filepath']).reset_index(drop=True)\n",
    "        print(f\"Keeping {len(df)} images with valid file paths\")\n",
    "    \n",
    "    # Verify some files exist\n",
    "    existing_files = df[\"filepath\"].apply(os.path.exists)\n",
    "    valid_files = existing_files.sum()\n",
    "    print(f\"Verified {valid_files} out of {len(df)} image files exist\")\n",
    "    \n",
    "    # Keep only existing files\n",
    "    df = df[existing_files].reset_index(drop=True)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"No valid image files found! Please check your directory structure and file paths.\")\n",
    "    \n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    for level in sorted(df['label'].unique()):\n",
    "        count = (df['label'] == level).sum()\n",
    "        class_name = class_names_5.get(level, f\"Unknown ({level})\")\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {level} ({class_name}): {count} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Updated main training pipeline\n",
    "def main_training_pipeline():\n",
    "    \"\"\"Main training pipeline with updated data loading\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CUDA-OPTIMIZED DIABETIC RETINOPATHY CLASSIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # GPU Information\n",
    "    if HAS_GPU:\n",
    "        print(f\"✓ Running on GPU with {len(GPU_DEVICES)} device(s)\")\n",
    "        print(f\"✓ Mixed precision: Enabled\")\n",
    "        print(f\"✓ XLA compilation: Enabled\")\n",
    "        print(f\"✓ Optimized batch size: {BATCH_SIZE}\")\n",
    "    else:\n",
    "        print(\"⚠ Running on CPU\")\n",
    "    \n",
    "    # Updated paths for new structure\n",
    "    BASE_DIR = r\"C:\\Users\\nande\\OneDrive\\Desktop\\Diabetic_Retinopathy\\DataBase\"\n",
    "    train_csv_path = os.path.join(BASE_DIR, \"train.csv\")  # CSV with image_id and level columns\n",
    "    images_folder_name = \"images\"  # Folder name containing all images (adjust as needed)\n",
    "    \n",
    "    # Load and prepare data with new structure\n",
    "    try:\n",
    "        df = load_and_prepare_data(BASE_DIR, train_csv_path, images_folder_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. CSV file exists and contains 'image_id' and 'level' columns\")\n",
    "        print(\"2. Images folder contains the actual image files\")\n",
    "        print(\"3. File paths and names are correct\")\n",
    "        return None\n",
    "    \n",
    "    # Train-validation-test split\n",
    "    print(\"\\nSplitting data into train/validation/test sets...\")\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, stratify=df[\"label\"], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
    "    \n",
    "    print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "    # Create optimized datasets\n",
    "    print(\"Creating GPU-optimized datasets...\")\n",
    "    train_set = create_optimized_dataset_with_masks(train_df, BATCH_SIZE, shuffle=True, cache=True)\n",
    "    valid_set = create_optimized_dataset_with_masks(val_df, BATCH_SIZE, cache=True)\n",
    "    test_set = create_optimized_dataset_with_masks(test_df, BATCH_SIZE, cache=True)\n",
    "    \n",
    "    # Create GPU-optimized model\n",
    "    print(\"Creating GPU-optimized DenseNet model...\")\n",
    "    model, base_model = create_retina_focused_densenet_gpu()\n",
    "    \n",
    "    print(f\"\\nModel Parameters:\")\n",
    "    print(f\"Total params: {model.count_params():,}\")\n",
    "    print(f\"Trainable params: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "    \n",
    "    monitor_gpu_memory()\n",
    "    \n",
    "    # Enhanced callbacks with GPU monitoring\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=10, \n",
    "            restore_best_weights=True, \n",
    "            verbose=1,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max'\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            verbose=1,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_retina_model_5class_gpu.keras', \n",
    "            save_best_only=True, \n",
    "            verbose=1,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_format='keras'\n",
    "        ),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: monitor_gpu_memory()\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 1: Training with frozen base\n",
    "    print(f\"\\nPhase 1: Training with frozen base model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        history1 = model.fit(\n",
    "            train_set,\n",
    "            validation_data=valid_set,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1,\n",
    "            workers=4 if not HAS_GPU else 1,  # Reduce workers for GPU\n",
    "            use_multiprocessing=False  # Disable multiprocessing for GPU\n",
    "        )\n",
    "    \n",
    "    phase1_time = time.time() - start_time\n",
    "    print(f\"Phase 1 training time: {phase1_time:.2f} seconds\")\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    if HAS_GPU:\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Evaluate Phase 1\n",
    "    results1 = evaluate_model_5_class_gpu(model, test_set, \"DenseNet + Retina Mask (Frozen)\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning with GPU optimization\n",
    "    print(f\"\\nPhase 2: Fine-tuning with GPU optimization...\")\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    n_layers = len(base_model.layers)\n",
    "    freeze_layers = int(0.8 * n_layers)  # Freeze more layers for stability\n",
    "    \n",
    "    for layer in base_model.layers[:freeze_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    unfrozen_layers = n_layers - freeze_layers\n",
    "    print(f\"Unfroze {unfrozen_layers} out of {n_layers} base model layers\")\n",
    "    \n",
    "    # Recompile with lower learning rate and gradient clipping\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-6,  # Lower learning rate for fine-tuning\n",
    "        clipnorm=1.0 if HAS_GPU else None  # Gradient clipping for stability\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "        run_eagerly=False\n",
    "    )\n",
    "    \n",
    "    # Updated callbacks for fine-tuning\n",
    "    callbacks_list_ft = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=12, \n",
    "            restore_best_weights=True, \n",
    "            verbose=1,\n",
    "            monitor='val_accuracy'\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.3, \n",
    "            patience=6, \n",
    "            verbose=1,\n",
    "            min_lr=1e-8\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_retina_model_5class_gpu_final.keras', \n",
    "            save_best_only=True, \n",
    "            verbose=1,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max'\n",
    "        ),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: monitor_gpu_memory()\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        history2 = model.fit(\n",
    "            train_set,\n",
    "            validation_data=valid_set,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks_list_ft,\n",
    "            verbose=1,\n",
    "            workers=4 if not HAS_GPU else 1,\n",
    "            use_multiprocessing=False\n",
    "        )\n",
    "    \n",
    "    phase2_time = time.time() - start_time\n",
    "    print(f\"Phase 2 training time: {phase2_time:.2f} seconds\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    results2 = evaluate_model_5_class_gpu(model, test_set, \"DenseNet + Retina Mask (Fine-tuned)\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = 'retina_focused_dr_classifier_5class_gpu.keras'\n",
    "    print(f\"\\nSaving final model to: {model_path}\")\n",
    "    model.save(model_path, save_format='keras')\n",
    "    print(f\"✓ Model saved successfully!\")\n",
    "    \n",
    "    # Training summary with GPU info\n",
    "    total_time = phase1_time + phase2_time\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"CUDA-OPTIMIZED TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    if HAS_GPU:\n",
    "        print(f\"GPU Acceleration: ENABLED\")\n",
    "        print(f\"Mixed Precision: ENABLED\")\n",
    "        print(f\"XLA Compilation: ENABLED\")\n",
    "        monitor_gpu_memory()\n",
    "    print(f\"Phase 1 time: {phase1_time:.2f} seconds\")\n",
    "    print(f\"Phase 2 time: {phase2_time:.2f} seconds\")\n",
    "    print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "    print(f\"Final model accuracy: {results2['overall_accuracy']:.4f}\")\n",
    "    print(f\"Final model kappa score: {results2['kappa_score']:.4f}\")\n",
    "    print(f\"Batch size used: {BATCH_SIZE}\")\n",
    "    print(f\"Model saved as: {model_path}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if 'results1' in locals() and 'results2' in locals():\n",
    "        improvement = results2['overall_accuracy'] - results1['overall_accuracy']\n",
    "        print(f\"Accuracy improvement from fine-tuning: {improvement:.4f}\")\n",
    "    \n",
    "    print(\"\\n✓ CUDA-optimized training completed successfully!\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    if HAS_GPU:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nTo use the trained model for prediction:\")\n",
    "    print(f\"model = tf.keras.models.load_model('{model_path}', custom_objects={{'RetinaMaskLayer': RetinaMaskLayer}})\")\n",
    "    \n",
    "    return model, train_df, val_df, test_df, history1, history2, results1, results2\n",
    "\n",
    "def predict_single_image_gpu(model, img_path, img_size=(224, 224)):\n",
    "    \"\"\"GPU-optimized single image prediction with visualization\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from {img_path}\")\n",
    "        return None, None, None\n",
    "        \n",
    "    img = cv2.resize(img, img_size)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create mask using GPU-optimized function\n",
    "    img_tensor = tf.constant(img_rgb, dtype=tf.float32)\n",
    "    \n",
    "    with tf.device('/GPU:0' if HAS_GPU else '/CPU:0'):\n",
    "        mask = create_retina_mask_gpu(img_tensor)\n",
    "        \n",
    "        # Preprocess for model\n",
    "        img_array = tf.keras.applications.densenet.preprocess_input(\n",
    "            tf.cast(img_tensor, tf.float32)\n",
    "        )\n",
    "        img_array = tf.expand_dims(img_array, axis=0)\n",
    "        mask_array = tf.expand_dims(tf.expand_dims(mask, axis=-1), axis=0)\n",
    "        \n",
    "        # GPU-optimized prediction\n",
    "        pred = model({'image': img_array, 'mask': mask_array}, training=False)\n",
    "        pred = pred.numpy()\n",
    "    \n",
    "    pred_class = pred.argmax(axis=1)[0]\n",
    "    confidence = pred.max(axis=1)[0]\n",
    "    \n",
    "    class_names = {0: \"No DR\", 1: \"Mild DR\", 2: \"Moderate DR\", 3: \"Severe DR\", 4: \"Proliferative DR\"}\n",
    "    \n",
    "    print(f\"Prediction: {class_names[pred_class]}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(f\"All probabilities: {pred[0]}\")\n",
    "    \n",
    "    # Visualize the image and mask\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(mask.numpy(), cmap='viridis')\n",
    "    plt.title('GPU-Generated Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    masked_img = img_rgb * np.expand_dims(mask.numpy(), axis=-1)\n",
    "    plt.imshow(masked_img.astype(np.uint8))\n",
    "    plt.title('Masked Retina Region')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    bars = plt.bar(range(5), pred[0])\n",
    "    plt.title(f'Prediction Probabilities\\n{class_names[pred_class]} ({confidence:.3f})')\n",
    "    plt.xticks(range(5), ['No DR', 'Mild', 'Mod.', 'Sev.', 'Prolif.'], rotation=45)\n",
    "    plt.ylabel('Probability')\n",
    "    \n",
    "    # Color the predicted class bar differently\n",
    "    colors = ['lightblue'] * 5\n",
    "    colors[pred_class] = 'orange'\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_class, confidence, pred[0]\n",
    "\n",
    "def benchmark_gpu_performance(model, test_set, num_batches=10):\n",
    "    \"\"\"Benchmark GPU vs CPU performance\"\"\"\n",
    "    print(\"\\nBenchmarking GPU Performance...\")\n",
    "    \n",
    "    # Take a subset of test data for benchmarking\n",
    "    test_batches = list(test_set.take(num_batches))\n",
    "    \n",
    "    if HAS_GPU:\n",
    "        # GPU timing\n",
    "        with tf.device('/GPU:0'):\n",
    "            start_time = time.time()\n",
    "            for batch_data, _ in test_batches:\n",
    "                _ = model(batch_data, training=False)\n",
    "            gpu_time = time.time() - start_time\n",
    "        \n",
    "        # CPU timing for comparison\n",
    "        with tf.device('/CPU:0'):\n",
    "            start_time = time.time()\n",
    "            for batch_data, _ in test_batches:\n",
    "                _ = model(batch_data, training=False)\n",
    "            cpu_time = time.time() - start_time\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        print(f\"GPU inference time: {gpu_time:.3f} seconds\")\n",
    "        print(f\"CPU inference time: {cpu_time:.3f} seconds\")\n",
    "        print(f\"GPU speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        return gpu_time, cpu_time, speedup\n",
    "    else:\n",
    "        # CPU only timing\n",
    "        start_time = time.time()\n",
    "        for batch_data, _ in test_batches:\n",
    "            _ = model(batch_data, training=False)\n",
    "        cpu_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"CPU inference time: {cpu_time:.3f} seconds\")\n",
    "        return cpu_time, None, None\n",
    "\n",
    "def create_gpu_training_report(history1, history2, results1, results2, total_time):\n",
    "    \"\"\"Generate comprehensive GPU training report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE GPU TRAINING REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # System Information\n",
    "    print(\"\\n1. SYSTEM CONFIGURATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    if HAS_GPU:\n",
    "        for i, gpu in enumerate(GPU_DEVICES):\n",
    "            print(f\"   GPU {i}: {gpu.name}\")\n",
    "        print(f\"   Mixed Precision: ENABLED\")\n",
    "        print(f\"   XLA Compilation: ENABLED\")\n",
    "        print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "    else:\n",
    "        print(\"   Device: CPU\")\n",
    "        print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "    print(f\"   Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"   Total Training Time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Training Performance\n",
    "    print(\"\\n2. TRAINING PERFORMANCE:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if history1:\n",
    "        phase1_epochs = len(history1.history['loss'])\n",
    "        final_train_acc1 = history1.history['accuracy'][-1]\n",
    "        final_val_acc1 = history1.history['val_accuracy'][-1]\n",
    "        \n",
    "        print(f\"   Phase 1 (Frozen Base):\")\n",
    "        print(f\"     Epochs Completed: {phase1_epochs}\")\n",
    "        print(f\"     Final Training Accuracy: {final_train_acc1:.4f}\")\n",
    "        print(f\"     Final Validation Accuracy: {final_val_acc1:.4f}\")\n",
    "    \n",
    "    if history2:\n",
    "        phase2_epochs = len(history2.history['loss'])\n",
    "        final_train_acc2 = history2.history['accuracy'][-1]\n",
    "        final_val_acc2 = history2.history['val_accuracy'][-1]\n",
    "        \n",
    "        print(f\"   Phase 2 (Fine-tuned):\")\n",
    "        print(f\"     Epochs Completed: {phase2_epochs}\")\n",
    "        print(f\"     Final Training Accuracy: {final_train_acc2:.4f}\")\n",
    "        print(f\"     Final Validation Accuracy: {final_val_acc2:.4f}\")\n",
    "    \n",
    "    # Model Performance\n",
    "    print(\"\\n3. MODEL EVALUATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if results1:\n",
    "        print(f\"   Phase 1 Results:\")\n",
    "        print(f\"     Test Accuracy: {results1['overall_accuracy']:.4f}\")\n",
    "        print(f\"     Cohen's Kappa: {results1['kappa_score']:.4f}\")\n",
    "    \n",
    "    if results2:\n",
    "        print(f\"   Phase 2 Results:\")\n",
    "        print(f\"     Test Accuracy: {results2['overall_accuracy']:.4f}\")\n",
    "        print(f\"     Cohen's Kappa: {results2['kappa_score']:.4f}\")\n",
    "        \n",
    "        if results1:\n",
    "            improvement = results2['overall_accuracy'] - results1['overall_accuracy']\n",
    "            print(f\"     Improvement: +{improvement:.4f}\")\n",
    "    \n",
    "    # GPU Utilization Analysis\n",
    "    print(\"\\n4. GPU UTILIZATION ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if HAS_GPU:\n",
    "        try:\n",
    "            gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            peak_mb = gpu_info['peak'] / (1024**2)\n",
    "            print(f\"   Peak GPU Memory Usage: {peak_mb:.1f} MB\")\n",
    "        except:\n",
    "            print(\"   GPU memory info unavailable\")\n",
    "    else:\n",
    "        print(\"   No GPU utilized\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def optimize_gpu_memory():\n",
    "    \"\"\"Optimize GPU memory usage\"\"\"\n",
    "    if HAS_GPU:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "            print(\"✓ GPU memory optimized\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def validate_data_structure(base_dir, csv_filename=\"train.csv\", images_folder=\"images\"):\n",
    "    \"\"\"\n",
    "    Validate the data structure and provide helpful information\n",
    "    \"\"\"\n",
    "    print(\"Validating data structure...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check base directory\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"❌ Base directory not found: {base_dir}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✓ Base directory exists: {base_dir}\")\n",
    "    \n",
    "    # Check CSV file\n",
    "    csv_path = os.path.join(base_dir, csv_filename)\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"❌ CSV file not found: {csv_path}\")\n",
    "        print(f\"Available files in base directory:\")\n",
    "        for f in os.listdir(base_dir):\n",
    "            if f.endswith('.csv'):\n",
    "                print(f\"  - {f}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✓ CSV file exists: {csv_path}\")\n",
    "    \n",
    "    # Check images directory\n",
    "    images_dir = os.path.join(base_dir, images_folder)\n",
    "    if not os.path.exists(images_dir):\n",
    "        print(f\"❌ Images directory not found: {images_dir}\")\n",
    "        print(f\"Available directories in base directory:\")\n",
    "        for item in os.listdir(base_dir):\n",
    "            if os.path.isdir(os.path.join(base_dir, item)):\n",
    "                print(f\"  - {item}/\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✓ Images directory exists: {images_dir}\")\n",
    "    \n",
    "    # Analyze CSV structure\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"✓ CSV loaded successfully with {len(df)} rows\")\n",
    "        print(f\"✓ CSV columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        has_image_id = 'image_id' in df.columns or 'id_code' in df.columns\n",
    "        has_level = 'level' in df.columns or 'diagnosis' in df.columns\n",
    "        \n",
    "        if has_image_id:\n",
    "            id_col = 'image_id' if 'image_id' in df.columns else 'id_code'\n",
    "            print(f\"✓ Image ID column found: {id_col}\")\n",
    "        else:\n",
    "            print(\"❌ No image ID column found (expected 'image_id' or 'id_code')\")\n",
    "            \n",
    "        if has_level:\n",
    "            level_col = 'level' if 'level' in df.columns else 'diagnosis'\n",
    "            print(f\"✓ Level column found: {level_col}\")\n",
    "            unique_levels = sorted(df[level_col].unique())\n",
    "            print(f\"✓ Unique levels: {unique_levels}\")\n",
    "        else:\n",
    "            print(\"❌ No level column found (expected 'level' or 'diagnosis')\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading CSV: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check images in directory\n",
    "    try:\n",
    "        image_files = os.listdir(images_dir)\n",
    "        num_images = len([f for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        print(f\"✓ Found {num_images} image files in directory\")\n",
    "        \n",
    "        if num_images > 0:\n",
    "            print(f\"Sample image files: {image_files[:5]}\")\n",
    "        else:\n",
    "            print(\"❌ No image files found in directory\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading images directory: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"✓ Data structure validation completed\")\n",
    "    return True\n",
    "\n",
    "# Updated main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration - UPDATE THESE PATHS FOR YOUR SETUP\n",
    "    BASE_DIR = r\"C:\\Users\\nande\\OneDrive\\Desktop\\Diabetic_Retinopathy\\DataBase\"\n",
    "    CSV_FILENAME = \"train.csv\"  # Your CSV file name\n",
    "    IMAGES_FOLDER = \"images\"    # Your images folder name (all images in one folder)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"UPDATED DIABETIC RETINOPATHY CLASSIFIER\")\n",
    "    print(\"Supporting new data structure with all images in one folder\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First validate the data structure\n",
    "    if not validate_data_structure(BASE_DIR, CSV_FILENAME, IMAGES_FOLDER):\n",
    "        print(\"\\n❌ Data structure validation failed!\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. Update BASE_DIR to your actual data directory\")\n",
    "        print(\"2. Update CSV_FILENAME to your actual CSV file name\")\n",
    "        print(\"3. Update IMAGES_FOLDER to your actual images folder name\")\n",
    "        print(\"4. CSV should contain 'image_id' (or 'id_code') and 'level' (or 'diagnosis') columns\")\n",
    "        print(\"5. All images should be in the specified images folder\")\n",
    "    else:\n",
    "        print(\"\\n✓ Data structure validation passed!\")\n",
    "        print(\"\\nStarting training pipeline...\")\n",
    "        \n",
    "        # Run the main training pipeline\n",
    "        try:\n",
    "            results = main_training_pipeline()\n",
    "            if results:\n",
    "                model, train_df, val_df, test_df, history1, history2, results1, results2 = results\n",
    "                \n",
    "                # Generate comprehensive report\n",
    "                total_time = 0  # This would be calculated in the actual pipeline\n",
    "                create_gpu_training_report(history1, history2, results1, results2, total_time)\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"\\nNext steps:\")\n",
    "                print(\"1. Use predict_single_image_gpu() for single image predictions\")\n",
    "                print(\"2. Use benchmark_gpu_performance() to test inference speed\")\n",
    "                print(\"3. Load the saved model for deployment\")\n",
    "                print(\"=\"*60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Training failed with error: {e}\")\n",
    "            print(\"\\nPlease check your data paths and try again.\")\n",
    "\n",
    "def demo_usage_examples():\n",
    "    \"\"\"Demonstrate usage examples for the updated classifier\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USAGE EXAMPLES FOR UPDATED CLASSIFIER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. VALIDATE YOUR DATA STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "# Update these paths for your setup\n",
    "BASE_DIR = r\"your/path/to/database\"\n",
    "CSV_FILENAME = \"your_labels.csv\"\n",
    "IMAGES_FOLDER = \"your_images_folder\"\n",
    "\n",
    "# Validate structure\n",
    "validate_data_structure(BASE_DIR, CSV_FILENAME, IMAGES_FOLDER)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n2. TRAIN THE MODEL:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "# Run the main training pipeline\n",
    "results = main_training_pipeline()\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n3. MAKE PREDICTIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\n",
    "    'retina_focused_dr_classifier_5class_gpu.keras',\n",
    "    custom_objects={'RetinaMaskLayer': RetinaMaskLayer}\n",
    ")\n",
    "\n",
    "# Predict single image\n",
    "pred_class, confidence, probabilities = predict_single_image_gpu(\n",
    "    model, \n",
    "    'path/to/your/image.jpg'\n",
    ")\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n4. BENCHMARK PERFORMANCE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "# Benchmark GPU vs CPU performance\n",
    "gpu_time, cpu_time, speedup = benchmark_gpu_performance(model, test_set)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UPDATED CUDA-OPTIMIZED DIABETIC RETINOPATHY CLASSIFIER LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(\"New Features for Updated Data Structure:\")\n",
    "print(\"• Flexible data loading from single images folder\")\n",
    "print(\"• Automatic image file extension detection\")\n",
    "print(\"• CSV-based label mapping with image_id\")\n",
    "print(\"• Data structure validation and debugging\")\n",
    "print(\"• Support for different CSV column names\")\n",
    "print(\"• Comprehensive error handling and guidance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nIMPORTANT: Update the paths in the main execution section!\")\n",
    "print(\"- BASE_DIR: Your database directory path\")\n",
    "print(\"- CSV_FILENAME: Your CSV file name (with image_id and level columns)\")\n",
    "print(\"- IMAGES_FOLDER: Your images folder name (containing all images)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
